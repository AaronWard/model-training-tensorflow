{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the model that will be used to train the deep convolutional neaural network.\n",
    "\n",
    "@Author : Aaron Ward \n",
    "'''\n",
    "import tensorflow as tf\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import skimage\n",
    "from skimage import data, io, filters\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline # sets the backend of matplotlib to the 'inline' backend\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basic import directories and parameters initialized\n",
    "\n",
    "Remember to use os.getcwd() initializing partha on local \n",
    "machine\n",
    "\n",
    "'''\n",
    "TRAINING_DIR = '/data/training'\n",
    "TESTING_DIR = '/data/testing'\n",
    "MODEL_PATH = '/output/trained_model.ckpt'\n",
    "SAVE = '/output/'\n",
    "\n",
    "# Define initial variables\n",
    "batch_size = 100\n",
    "num_class = 6\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read...\n"
     ]
    }
   ],
   "source": [
    "####################################### DATA PREPROCESSING - Labeling ################################################\n",
    "'''\n",
    "This function traverses throwe ach directory of training images\n",
    "Two lists are made:\n",
    "    - The RGB image values are added to the images list\n",
    "    - For every photo in say the 'angry' directory of images, a \n",
    "      corresponding label is added to the label list\n",
    "\n",
    "'''\n",
    "def load_data(TRAINING_DIR):\n",
    "    images = []\n",
    "    labels = []\n",
    "    directories = [d for d in os.listdir(TRAINING_DIR) \n",
    "                if os.path.isdir(os.path.join(TRAINING_DIR, d))]\n",
    "    # Need to sort these because\n",
    "    # floyd hum jumbled up the order\n",
    "    directories = sorted(directories, key=int)\n",
    "\n",
    "    # Traverse through each directory and make a list\n",
    "    # of files names if they end in the PNG format\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(TRAINING_DIR, d)\n",
    "        file_names = [os.path.join(label_directory, f) \n",
    "                        for f in os.listdir(label_directory) \n",
    "                          if f.endswith(\".png\")]\n",
    "        #Traverse through each file, add the image data\n",
    "        # and label to the 2 lists\n",
    "        for f in file_names:\n",
    "            images.append(skimage.data.imread(f))\n",
    "            labels.append(int(d))\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_data(TRAINING_DIR)\n",
    "\n",
    "print('Data impored...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Shuffle the entire dataset and labels\n",
    "\n",
    "'''\n",
    "from sklearn.utils import shuffle\n",
    "images, labels = shuffle(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  1.  0.]\n",
      "[ 0.  0.  0.  0.  0.  1.]\n",
      "[ 0.  0.  0.  0.  1.  0.]\n",
      "[ 0.  0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell is  for converting the lists to numpy arrays \n",
    "\n",
    "'''\n",
    "num_images = len(images)\n",
    "images = np.array(images, object)\n",
    "labels = np.array(labels, dtype = np.int32)\n",
    "\n",
    "_labels = np.zeros((num_images, num_class))\n",
    "_labels[np.arange(num_images), labels] = 1.0\n",
    "labels = _labels\n",
    "\n",
    "print(labels[1])\n",
    "print(labels[2])\n",
    "print(labels[3])\n",
    "print(labels[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported...\n"
     ]
    }
   ],
   "source": [
    "####################################### DATA PREPROCESSING - Labeling ################################################\n",
    "'''\n",
    "import test data and labels\n",
    "'''\n",
    "def load_test_data(TESTING_DIR):\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    directories = [d for d in os.listdir(TESTING_DIR) \n",
    "                if os.path.isdir(os.path.join(TESTING_DIR, d))]\n",
    "    # Need to sort these because\n",
    "    # floyd hum jumbled up the order\n",
    "    directories = sorted(directories, key=int)\n",
    "\n",
    "    # Traverse through each directory and make a list\n",
    "    # of files names if they end in the PNG format\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(TESTING_DIR, d)\n",
    "        file_names = [os.path.join(label_directory, f) \n",
    "                        for f in os.listdir(label_directory) \n",
    "                          if f.endswith(\".png\")]\n",
    "        #Traverse through each file, add the image data\n",
    "        # and label to the 2 lists\n",
    "        for f in file_names:\n",
    "            test_images.append(skimage.data.imread(f))\n",
    "            test_labels.append(int(d))\n",
    "\n",
    "    return test_images, test_labels\n",
    "\n",
    "test_images, test_labels = load_data(TESTING_DIR)\n",
    "\n",
    "test_images = np.array(test_images, object)\n",
    "test_labels = np.array(test_labels, object)\n",
    "\n",
    "# Convert labels into a one hot vector \n",
    "test_labels = pd.get_dummies(test_labels)\n",
    "print('imported...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down scaling train images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down scaling test images...\n",
      "Images Downscaled...\n"
     ]
    }
   ],
   "source": [
    "####################################### DATA PREPROCESSING - Imaging #######################################\n",
    "'''\n",
    "This cell is for image downsampling and transformation\n",
    "This is on the fly to resize the images to a 50x50 size\n",
    "\n",
    "'''\n",
    "from skimage import transform, exposure\n",
    "\n",
    "print('Down scaling train images...')\n",
    "images = [transform.resize(image, (50, 50)) for image in images]\n",
    "\n",
    "print('Down scaling test images...')\n",
    "test_images = [transform.resize(test_image, (50, 50)) for test_image in test_images]\n",
    "\n",
    "print('Images Downscaled...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell is for initializing variables for the tensorflow session and \n",
    "placeholders for holding the data.\n",
    "\n",
    "'''\n",
    "# Initialize placeholders \n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 50, 50], name='X_placeholder')\n",
    "y = tf.placeholder(dtype = tf.int32, shape= [None, num_class], name=\"Y_placeholder\")\n",
    "is_training = tf.placeholder( dtype = tf.bool, shape = (), name = \"is_training\" )\n",
    "\n",
    "#define variables for dropout\n",
    "keep_rate = .8\n",
    "keep_prop = tf.placeholder(tf.float32)\n",
    "print('initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network defined...\n"
     ]
    }
   ],
   "source": [
    "######################################### NETWORK STRUCTURE #################################################\n",
    "'''\n",
    "This cell is for defining the stucture of the neural network.\n",
    "The network has 11 convolutional layers and 2 fully connected layers\n",
    "\n",
    "'''\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def convolutional_network(x, is_training):\n",
    "    conv_net = tf.reshape(x, shape=[-1, 50, 50, 1]) # add channel dimensions\n",
    "\n",
    "    #used for scoping layers arguments\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "            padding = \"SAME\",\n",
    "            activation_fn = tf.nn.relu,\n",
    "            stride = 1,\n",
    "            weights_initializer = tf.truncated_normal_initializer(stddev=0.01),\n",
    "            weights_regularizer = slim.l2_regularizer(0.0005),\n",
    "            normalizer_fn = slim.batch_norm,\n",
    "            normalizer_params = {'scale' : True,'trainable' : False, 'is_training' : is_training }):\n",
    "        \n",
    "        conv_net = slim.conv2d(conv_net, 32, 3)\n",
    "        conv_net = slim.conv2d(conv_net, 64, 3)\n",
    "        conv_net = slim.conv2d(conv_net, 64, 3)\n",
    "        conv_net = slim.max_pool2d(conv_net, 3, stride = 1 )\n",
    "        conv_net = slim.conv2d(conv_net, 96, 3)\n",
    "        conv_net = slim.conv2d(conv_net, 96, 3)\n",
    "        conv_net = slim.max_pool2d(conv_net, 2, stride = 2)\n",
    "        \n",
    "        conv_net = slim.conv2d(conv_net, 128, 3)\n",
    "        conv_net = slim.conv2d(conv_net, 128, 3)\n",
    "        conv_net = slim.max_pool2d(conv_net, 2, stride = 2)\n",
    "\n",
    "        conv_net = slim.conv2d(conv_net, 128, 3)\n",
    "        conv_net = slim.conv2d(conv_net, 128, 3)\n",
    "        conv_net = slim.max_pool2d(conv_net, 2, stride = 2)\n",
    "\n",
    "        conv_net = slim.conv2d(conv_net, 128, 3)\n",
    "        conv_net = slim.max_pool2d(conv_net, 2, stride = 1)\n",
    "        \n",
    "        conv_net = slim.dropout(conv_net, keep_prob = keep_rate, is_training = is_training )\n",
    "\n",
    "    # Fully Connect Layer\n",
    "    with slim.arg_scope([slim.fully_connected ],weights_regularizer = slim.l2_regularizer(0.0005)):\n",
    "        conv_net = slim.flatten(conv_net)\n",
    "        output = slim.fully_connected(conv_net, num_class, activation_fn = None)\n",
    "        prediction = tf.nn.softmax(output)\n",
    "\n",
    "        return output, prediction\n",
    "    \n",
    "print('Network defined...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Shuffle the batches on the fly.\n",
    "\n",
    "'''\n",
    "\n",
    "def randomize(batch_x, batch_y):\n",
    "    batch_x, batch_y = shuffle(batch_x, batch_y)\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################### NETWORK TRAINING #################################################\n",
    "'''\n",
    "This cell contains a function that runs the tensorflow session, it is called with the x placeholders.\n",
    "The session is ran by first initializing all the tensorflow variables, then iterated through\n",
    "the number of epochs and feed the image data and labels using feed_dict.\n",
    "The loss/cost and accuracy is evaluated and printed to the console.\n",
    "\n",
    "'''\n",
    "\n",
    "def train_network(x):\n",
    "    output, prediction = convolutional_network(x, is_training)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "    total_losses = tf.losses.get_total_loss( add_regularization_losses=True ) + loss\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # needed for batch normalization\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = tf.train.AdamOptimizer( learning_rate=0.002 ).minimize(total_losses)\n",
    "    \n",
    "    correct = tf.equal(tf.argmax(prediction,1),  tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer()) # Initialize all the variables\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        time_full_start = time.clock()\n",
    "        \n",
    "        print(\"RUNNING SESSION...\")\n",
    "        for epoch in range(num_epochs):\n",
    "            train_batch_x = []\n",
    "            train_batch_y = []\n",
    "            epoch_loss= 0\n",
    "            epoch_total_loss = 0\n",
    "            accuracy = 0\n",
    "            time_epoch_start = time.clock()\n",
    "            i = 0\n",
    "            number_of_batches = 0\n",
    "            \n",
    "            #For all images in the DS, batch into sizes of 100\n",
    "            while i < len(images):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                train_batch_x = images[start:end]\n",
    "                train_batch_y = labels[start:end]\n",
    "                \n",
    "                #Randomize the batches even more\n",
    "                train_batch_x, train_batch_y = randomize(train_batch_x, train_batch_y)\n",
    "                \n",
    "                #Feed batches into tensorflow\n",
    "                op, ac, loss_value, total_loss_value = sess.run([train_op, acc, loss, total_losses],feed_dict={x: train_batch_x,\n",
    "                                                                    y: train_batch_y, is_training : True})\n",
    "                epoch_loss += loss_value\n",
    "                epoch_total_loss += total_loss_value\n",
    "                accuracy += ac\n",
    "                i += batch_size\n",
    "                number_of_batches += 1\n",
    "            \n",
    "            accuracy /= number_of_batches\n",
    "            \n",
    "            print('Epoch:', epoch+1, 'total loss: ', epoch_total_loss  ,' loss: ', epoch_loss ,' acc: {: %}'.format(accuracy))\n",
    "            \n",
    "            time_epoch_end = time.clock()\n",
    "            print('Time elapse: ', time_epoch_end - time_epoch_start)\n",
    "\n",
    "        time_full_end = time.clock()\n",
    "        print('Full time elapse:', time_full_end - time_full_start)\n",
    "\n",
    "        if epoch_loss < 100:\n",
    "            save_path = saver.save(sess, MODEL_PATH)\n",
    "            print(\"Model saved in file: \" , save_path)\n",
    "\n",
    "        print('Accuracy:', acc.eval({x: test_images, y: test_labels, is_training : False }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SESSION...\n",
      "Epoch: 1 total loss:  183.691210866  loss:  179.716000438  acc: 23.630952%\n",
      "Time elapse:  33.010194\n",
      "Epoch: 2 total loss:  91.9507848024  loss:  87.3788096905  acc: 59.916664%\n",
      "Time elapse:  30.108475\n",
      "Epoch: 3 total loss:  46.6742544174  loss:  41.7595326006  acc: 82.702380%\n",
      "Time elapse:  30.44146999999998\n",
      "Epoch: 4 total loss:  32.0835555047  loss:  26.785487242  acc: 88.797621%\n",
      "Time elapse:  30.84901099999999\n",
      "Epoch: 5 total loss:  24.064837262  loss:  18.3768203631  acc: 92.428574%\n",
      "Time elapse:  30.754599999999982\n",
      "Epoch: 6 total loss:  18.3700393215  loss:  12.3525288198  acc: 94.750003%\n",
      "Time elapse:  30.903415999999993\n",
      "Epoch: 7 total loss:  15.4833662733  loss:  9.13755525835  acc: 96.369050%\n",
      "Time elapse:  30.970293999999967\n",
      "Epoch: 8 total loss:  15.4407610819  loss:  8.89202417247  acc: 96.511907%\n",
      "Time elapse:  30.97257000000002\n",
      "Epoch: 9 total loss:  15.5944260433  loss:  8.70499422774  acc: 96.642859%\n",
      "Time elapse:  30.890907000000027\n",
      "Epoch: 10 total loss:  11.7807634845  loss:  4.79466013378  acc: 98.142858%\n",
      "Time elapse:  30.62919099999999\n",
      "Epoch: 11 total loss:  12.856492199  loss:  5.86537222844  acc: 97.785716%\n",
      "Time elapse:  30.48162099999996\n",
      "Epoch: 12 total loss:  12.6646487564  loss:  5.56463168073  acc: 97.892858%\n",
      "Time elapse:  31.126416000000006\n",
      "Epoch: 13 total loss:  12.4836546481  loss:  5.23060582764  acc: 97.916668%\n",
      "Time elapse:  30.45817199999999\n",
      "Epoch: 14 total loss:  12.9424832389  loss:  5.52058196999  acc: 97.904763%\n",
      "Time elapse:  31.00837200000001\n",
      "Epoch: 15 total loss:  11.8527888805  loss:  4.30100097694  acc: 98.250001%\n",
      "Time elapse:  30.733934999999974\n",
      "Epoch: 16 total loss:  12.328789793  loss:  4.8006234105  acc: 98.166668%\n",
      "Time elapse:  31.052096000000006\n",
      "Epoch: 17 total loss:  14.4125062078  loss:  6.65449227009  acc: 97.250002%\n",
      "Time elapse:  30.92013899999995\n",
      "Epoch: 18 total loss:  13.7399358973  loss:  5.46325978194  acc: 98.023811%\n",
      "Time elapse:  30.762743\n",
      "Epoch: 19 total loss:  17.2794813439  loss:  8.75893089897  acc: 96.773811%\n",
      "Time elapse:  30.592052999999964\n",
      "Epoch: 20 total loss:  17.1013278589  loss:  7.6973828848  acc: 97.035716%\n",
      "Time elapse:  30.95978200000002\n",
      "Epoch: 21 total loss:  14.290398851  loss:  4.62734748283  acc: 98.238096%\n",
      "Time elapse:  30.966827999999964\n",
      "Epoch: 22 total loss:  13.3648257405  loss:  3.84829020634  acc: 98.619049%\n",
      "Time elapse:  30.384198999999967\n",
      "Epoch: 23 total loss:  13.0968182757  loss:  3.80248674093  acc: 98.630953%\n",
      "Time elapse:  30.576471999999967\n",
      "Epoch: 24 total loss:  13.8080659956  loss:  4.4965007758  acc: 98.154763%\n",
      "Time elapse:  30.642255999999975\n",
      "Epoch: 25 total loss:  12.4715422243  loss:  3.30310567864  acc: 98.940477%\n",
      "Time elapse:  31.006574\n",
      "Epoch: 26 total loss:  11.9188642651  loss:  3.03766347273  acc: 98.857144%\n",
      "Time elapse:  30.90097700000001\n",
      "Epoch: 27 total loss:  14.2775650695  loss:  5.20399207086  acc: 98.059525%\n",
      "Time elapse:  30.99240300000008\n",
      "Epoch: 28 total loss:  16.8356417418  loss:  6.98281846452  acc: 97.702383%\n",
      "Time elapse:  30.846261000000027\n",
      "Epoch: 29 total loss:  15.629666023  loss:  5.46235528367  acc: 98.071430%\n",
      "Time elapse:  30.81596300000001\n",
      "Epoch: 30 total loss:  18.8644626737  loss:  7.95028258616  acc: 97.392858%\n",
      "Time elapse:  30.712873999999942\n",
      "Epoch: 31 total loss:  16.5648769736  loss:  5.15038908215  acc: 98.250001%\n",
      "Time elapse:  30.527195000000006\n",
      "Epoch: 32 total loss:  15.5956628025  loss:  4.33976549376  acc: 98.452382%\n",
      "Time elapse:  30.702849000000015\n",
      "Epoch: 33 total loss:  14.9794891626  loss:  4.08326958679  acc: 98.678572%\n",
      "Time elapse:  30.589165999999977\n",
      "Epoch: 34 total loss:  15.5135501921  loss:  4.6603820317  acc: 98.250001%\n",
      "Time elapse:  30.80420099999992\n",
      "Epoch: 35 total loss:  15.6924167573  loss:  4.77530353382  acc: 98.309525%\n",
      "Time elapse:  30.977333000000044\n",
      "Epoch: 36 total loss:  20.3649389744  loss:  8.46171848295  acc: 96.964287%\n",
      "Time elapse:  30.862802999999985\n",
      "Epoch: 37 total loss:  18.1637714058  loss:  5.26564301306  acc: 98.142858%\n",
      "Time elapse:  30.389127000000144\n",
      "Epoch: 38 total loss:  17.0647630095  loss:  4.26384499227  acc: 98.642858%\n",
      "Time elapse:  30.764566999999943\n",
      "Epoch: 39 total loss:  17.6169067025  loss:  5.00857259118  acc: 98.202382%\n",
      "Time elapse:  30.64261600000009\n",
      "Epoch: 40 total loss:  20.2521119416  loss:  7.38677664116  acc: 97.583335%\n",
      "Time elapse:  30.96268000000009\n",
      "Epoch: 41 total loss:  19.9392333627  loss:  6.00807042513  acc: 97.952383%\n",
      "Time elapse:  30.82986200000005\n",
      "Epoch: 42 total loss:  19.3506572545  loss:  5.21151837098  acc: 98.250001%\n",
      "Time elapse:  30.990874000000076\n",
      "Epoch: 43 total loss:  20.8771846145  loss:  6.68413458311  acc: 97.976192%\n",
      "Time elapse:  30.96216000000004\n",
      "Epoch: 44 total loss:  21.4921816736  loss:  6.76095358905  acc: 97.857144%\n",
      "Time elapse:  30.896068999999898\n",
      "Epoch: 45 total loss:  19.3882219791  loss:  4.79265057028  acc: 98.464287%\n",
      "Time elapse:  30.520860999999968\n",
      "Epoch: 46 total loss:  19.7635523677  loss:  5.4122879926  acc: 98.369049%\n",
      "Time elapse:  30.963032999999996\n",
      "Epoch: 47 total loss:  19.1997452527  loss:  4.86770191466  acc: 98.440477%\n",
      "Time elapse:  30.817927000000054\n",
      "Epoch: 48 total loss:  19.1753239632  loss:  5.07644341467  acc: 98.345239%\n",
      "Time elapse:  30.97143600000004\n",
      "Epoch: 49 total loss:  21.9493413717  loss:  6.96059974446  acc: 97.738096%\n",
      "Time elapse:  30.931151999999884\n",
      "Epoch: 50 total loss:  24.7488485575  loss:  8.57836405747  acc: 97.309525%\n",
      "Time elapse:  30.94737699999996\n",
      "Epoch: 51 total loss:  24.5987313539  loss:  7.47795647429  acc: 97.738097%\n",
      "Time elapse:  30.656653000000006\n",
      "Epoch: 52 total loss:  24.3808595836  loss:  6.75084377127  acc: 97.988096%\n",
      "Time elapse:  30.281637000000046\n",
      "Epoch: 53 total loss:  21.0647333562  loss:  4.26896163051  acc: 98.785715%\n",
      "Time elapse:  30.649548000000095\n",
      "Epoch: 54 total loss:  19.6521990299  loss:  3.96936490087  acc: 98.642858%\n",
      "Time elapse:  30.625219000000016\n",
      "Epoch: 55 total loss:  20.862447992  loss:  5.37024037798  acc: 98.047620%\n",
      "Time elapse:  31.042345999999952\n",
      "Epoch: 56 total loss:  26.7025569975  loss:  10.0544594751  acc: 97.202382%\n",
      "Time elapse:  30.63810699999999\n",
      "Epoch: 57 total loss:  22.7577890456  loss:  4.99217880184  acc: 98.488096%\n",
      "Time elapse:  30.833073000000013\n",
      "Epoch: 58 total loss:  24.8063292056  loss:  7.48641484998  acc: 97.821430%\n",
      "Time elapse:  30.780448999999862\n",
      "Epoch: 59 total loss:  26.3942923695  loss:  8.52215059375  acc: 97.488097%\n",
      "Time elapse:  30.798767999999882\n",
      "Epoch: 60 total loss:  23.7782776654  loss:  5.07230531424  acc: 98.369049%\n",
      "Time elapse:  30.864372999999887\n",
      "Epoch: 61 total loss:  21.349508211  loss:  3.90337386577  acc: 98.821429%\n",
      "Time elapse:  30.96989899999994\n",
      "Epoch: 62 total loss:  22.3442027718  loss:  5.77445406043  acc: 98.047620%\n",
      "Time elapse:  30.74392899999998\n",
      "Epoch: 63 total loss:  21.5028526932  loss:  4.60034546128  acc: 98.297620%\n",
      "Time elapse:  30.743601000000126\n",
      "Epoch: 64 total loss:  21.0834756345  loss:  4.73703776102  acc: 98.357144%\n",
      "Time elapse:  30.632209000000103\n",
      "Epoch: 65 total loss:  31.4957273006  loss:  13.0807834274  acc: 96.535717%\n",
      "Time elapse:  30.815110000000004\n",
      "Epoch: 66 total loss:  25.6975291073  loss:  5.48237677923  acc: 98.226192%\n",
      "Time elapse:  30.31985900000018\n",
      "Epoch: 67 total loss:  25.2752106786  loss:  6.50807780086  acc: 98.059525%\n",
      "Time elapse:  30.922977000000174\n",
      "Epoch: 68 total loss:  24.5030002594  loss:  5.58698452143  acc: 98.380953%\n",
      "Time elapse:  30.80999500000007\n",
      "Epoch: 69 total loss:  22.4669749141  loss:  4.35824755447  acc: 98.583334%\n",
      "Time elapse:  30.699364999999943\n",
      "Epoch: 70 total loss:  22.3781141639  loss:  5.06952571671  acc: 98.452382%\n",
      "Time elapse:  30.461181000000124\n",
      "Epoch: 71 total loss:  21.6224345416  loss:  4.6312193972  acc: 98.511906%\n",
      "Time elapse:  31.076681000000008\n",
      "Epoch: 72 total loss:  24.7932772189  loss:  7.3109431559  acc: 97.750002%\n",
      "Time elapse:  30.392365000000154\n",
      "Epoch: 73 total loss:  26.8017885387  loss:  7.16540534415  acc: 97.916668%\n",
      "Time elapse:  30.894480999999814\n",
      "Epoch: 74 total loss:  27.5911262929  loss:  7.62053279311  acc: 97.845239%\n",
      "Time elapse:  30.584482000000207\n",
      "Epoch: 75 total loss:  24.2440977544  loss:  4.78368070579  acc: 98.595239%\n",
      "Time elapse:  30.936463999999887\n",
      "Epoch: 76 total loss:  23.6562884599  loss:  5.32863404455  acc: 98.404763%\n",
      "Time elapse:  30.727770999999848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 total loss:  25.9848719388  loss:  6.90865685468  acc: 97.809525%\n",
      "Time elapse:  30.737685000000056\n",
      "Epoch: 78 total loss:  24.3245100826  loss:  5.39625577524  acc: 98.380954%\n",
      "Time elapse:  31.028099999999995\n",
      "Epoch: 79 total loss:  25.4272532314  loss:  6.44976210146  acc: 98.142858%\n",
      "Time elapse:  30.97952499999974\n",
      "Epoch: 80 total loss:  25.4335781336  loss:  6.34281130281  acc: 98.059525%\n",
      "Time elapse:  30.28812899999957\n",
      "Epoch: 81 total loss:  23.9286348522  loss:  4.86932051036  acc: 98.404763%\n",
      "Time elapse:  30.712856999999985\n",
      "Epoch: 82 total loss:  26.8374519795  loss:  7.4708617577  acc: 97.750001%\n",
      "Time elapse:  31.045665999999983\n",
      "Epoch: 83 total loss:  23.3944453001  loss:  3.91859696023  acc: 98.809525%\n",
      "Time elapse:  30.4315789999996\n",
      "Epoch: 84 total loss:  22.7124064118  loss:  4.83566517314  acc: 98.571430%\n",
      "Time elapse:  30.993156999999883\n",
      "Epoch: 85 total loss:  27.360463351  loss:  8.18776073506  acc: 97.678573%\n",
      "Time elapse:  30.956302999999934\n",
      "Epoch: 86 total loss:  25.6836081892  loss:  5.4400163508  acc: 98.202382%\n",
      "Time elapse:  31.03630499999963\n",
      "Epoch: 87 total loss:  26.9742141664  loss:  6.73805522221  acc: 98.142858%\n",
      "Time elapse:  30.92224999999962\n",
      "Epoch: 88 total loss:  24.6549897194  loss:  5.23298898192  acc: 98.571430%\n",
      "Time elapse:  30.99985400000014\n",
      "Epoch: 89 total loss:  24.880546093  loss:  6.10755258263  acc: 98.166668%\n",
      "Time elapse:  30.82834099999991\n",
      "Epoch: 90 total loss:  25.8351140618  loss:  5.88917997107  acc: 98.107144%\n",
      "Time elapse:  30.895324999999957\n",
      "Epoch: 91 total loss:  27.3622015268  loss:  7.42068323909  acc: 98.023811%\n",
      "Time elapse:  30.570506999999907\n",
      "Epoch: 92 total loss:  30.0035024881  loss:  8.58850712707  acc: 97.440478%\n",
      "Time elapse:  30.834644000000026\n",
      "Epoch: 93 total loss:  27.1823287606  loss:  5.60775495216  acc: 98.392858%\n",
      "Time elapse:  30.87514999999985\n",
      "Epoch: 94 total loss:  25.3300253749  loss:  4.79920686739  acc: 98.523810%\n",
      "Time elapse:  30.888394999999946\n",
      "Epoch: 95 total loss:  26.9374340028  loss:  6.38940102005  acc: 98.154763%\n",
      "Time elapse:  30.694222999999965\n",
      "Epoch: 96 total loss:  30.0381126404  loss:  9.41411965325  acc: 97.547621%\n",
      "Time elapse:  30.650543000000198\n",
      "Epoch: 97 total loss:  25.8572928458  loss:  4.05344215302  acc: 98.785715%\n",
      "Time elapse:  29.966235000000324\n",
      "Epoch: 98 total loss:  25.6154600531  loss:  5.60875948909  acc: 98.380953%\n",
      "Time elapse:  30.445838999999978\n",
      "Epoch: 99 total loss:  26.7075828314  loss:  6.64831400808  acc: 97.976192%\n",
      "Time elapse:  30.535813999999846\n",
      "Epoch: 100 total loss:  24.5696557462  loss:  3.99035489558  acc: 98.714287%\n",
      "Time elapse:  30.748273000000154\n",
      "Full time elapse: 3078.5304859999997\n",
      "Model saved in file:  /output/trained_model.ckpt\n",
      "Accuracy: 0.822857\n"
     ]
    }
   ],
   "source": [
    "train_network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################### NETWORK TESTING #################################################\n",
    "'''\n",
    "Simple  cell for loading the model and testing the labels predicted for a range of\n",
    "images\n",
    "'''\n",
    "def test(x):\n",
    "    output, prediction = convolutional_network_v2(x, False )\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.import_meta_graph('/output/trained_model.ckpt.meta')\n",
    "        saver.restore(sess, '/output/trained_model.ckpt' )\n",
    "        print('session restored...')\n",
    "\n",
    "        pred_ = tf.nn.softmax(output)\n",
    "\n",
    "        predicted = sess.run(pred_, feed_dict={x: test_images[400:410]})[0]\n",
    "        \n",
    "        print('Actual Labels for ten images\\n', test_labels[400:410])\n",
    "        print('\\nPredicited Labels for ten images\\n', predicted[400:410])\n",
    "test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
